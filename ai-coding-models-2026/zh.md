# AI 编程模型：OpenAI vs Anthropic — 2026 完整版图

## 摘要

截至 2026 年 2 月 7 日，Anthropic 和 OpenAI 正处于有史以来最激烈的 AI 编程竞赛中。两天前（2 月 5 日），两家公司在 20 分钟内先后发布了竞争模型：Claude Opus 4.6 和 GPT-5.3-Codex。没有一家公司拥有全面领先优势——各自在不同的基准、工作流和开发者群体中占主导。

**核心结论：** Claude 在 SWE-bench Verified（补丁生成）上领先。GPT-5.3-Codex 在 Terminal-Bench 2.0（代理终端工作流）上领先。竞争已从"哪个模型更聪明"转向"哪个工具/工作流更高效"。

---

## 1. 2026 年 2 月 5 日正面对决

| 基准 | Claude Opus 4.6 | GPT-5.3-Codex | 胜者 |
|------|-----------------|---------------|------|
| SWE-bench Verified | **80.8%** | 未报告 | Claude |
| SWE-bench Pro Public | 未报告 | **56.8%** | GPT-5.3 |
| Terminal-Bench 2.0 | 65.4% | **77.3%** | GPT-5.3 (+12pp) |
| OSWorld-Verified | **72.7%** | 64.7% | Claude (+8pp) |
| GPQA Diamond | **77.3%** | 73.8% | Claude |
| MMLU Pro | **85.1%** | 82.9% | Claude |
| GDPval-AA (Elo) | **1,606** | ~1,462 | Claude (+144 Elo) |
| Cybersecurity CTF | — | **77.6%** | GPT-5.3 |

**关键警告：** Anthropic 报告 SWE-bench Verified，OpenAI 报告 SWE-bench Pro Public。不同变体、不同题集。这是刻意策略——各自在偏好指标上宣称"领先"。

---

## 2. 完整模型时间线

### OpenAI（2025 年 4 月 — 2026 年 2 月）

| 日期 | 模型 | SWE-bench Verified | 核心特性 |
|------|------|--------------------|---------|
| 4/14 | GPT-4.1 / mini / nano | 54.6% | API 编程模型，100 万上下文，$0.10-$2.00/MTok |
| 4/16 | o3 | 69.1% | 推理模型，Codeforces 2727 Elo |
| 4/16 | o4-mini | 68.1% | 高性价比推理，Codeforces 2719 |
| 6/10 | o3-pro | — | 最大算力，$20/$80 每 MTok |
| 6/10 | o3 降价 | — | 降 80%：$10/$40 → $2/$8 |
| 8/7 | GPT-5 | 74.9% | 统一旗舰，比 o3 少用 22% token |
| 11/12 | GPT-5.1 + Codex-Max | 77.9% | 长周期编程，24 小时自主任务 |
| 12/18 | GPT-5.2 + Codex | 75.4% | 上下文压缩，SWE-bench Pro 56.4% |
| 2/5 | **GPT-5.3-Codex** | — | Terminal-Bench 77.3%，快 25% |

### Anthropic（2025 年 5 月 — 2026 年 2 月）

| 日期 | 模型 | SWE-bench Verified | 核心特性 |
|------|------|--------------------|---------|
| 5/22 | Opus 4 | 72.5% | 首个 Claude 4，发布时"最强编程模型" |
| 5/22 | Sonnet 4 | — | 混合推理（即时 + 扩展思考） |
| 8/5 | Opus 4.1 | 74.5% | 多文件重构 |
| 9 月 | Sonnet 4.5 | 77.2% | 代理工作流，并行计算可达 82% |
| 10 月 | Haiku 4.5 | 73.3% | 小模型匹配 Opus 4 |
| 11/24 | Opus 4.5 | **80.9%** | 首个 >80%，降价 67% |
| 2/5 | **Opus 4.6** | 80.8% | 100 万上下文，Agent Teams，发现 500+ 零日漏洞 |

---

## 3. OpenAI 深度分析

### GPT-4.1 家族 — 编程主力

API 专用。三种规格（GPT-4.1 / mini / nano）。100 万 token 上下文。设计决策：将"编程"与"推理"分离——GPT-4.1 做日常编程，o3 做复杂推理。双轨持续到 GPT-5 合并两者。

| 特性 | GPT-4.1 | mini | nano |
|------|---------|------|------|
| 上下文 | 100 万 | 100 万 | 100 万 |
| SWE-bench | 54.6% | — | — |
| 输入/MTok | $2.00 | $0.40 | $0.10 |
| 输出/MTok | $8.00 | $1.60 | $0.40 |

比 GPT-4o 高 60%（Windsurf 基准）。工具调用效率高 30%。发布当天入驻 Copilot。

### o3 / o4-mini — 推理编程

o3 先思考再编码——扩展思维链用于多步重构和复杂 bug 链。Codeforces 2727 Elo（超过 OpenAI 首席科学家 2665）。权衡：慢，每请求数分钟。

o4-mini 在 Codeforces 上击败 o3（2719 vs 2706），成本低约 50%。竞赛编程性价比更高，但真实世界 SWE-bench o3 仍占优（69.1% vs 68.1%）。

### GPT-5 — 代际飞跃（2025 年 8 月）

替代 GPT-4o / o3 / o4-mini / GPT-4.1 成为 ChatGPT 默认。核心洞察：比 o3 结果更好，同时**少用 22% token** 和 **45% 工具调用**。Token 效率直接转化为代理成本节省。$1.25/$10.00 每 MTok。

### GPT-5.3-Codex — 最新（2026 年 2 月 5 日）

| 基准 | GPT-5.3-Codex | vs GPT-5.2 |
|------|---------------|------------|
| Terminal-Bench 2.0 | 77.3% | **+13.3pp** |
| OSWorld-Verified | 64.7% | **+26.5pp** |
| Cybersecurity CTF | 77.6% | +10.2pp |
| SWE-bench Pro | 56.8% | +0.4pp |

推理快 25%。输出 token 比任何前代都少。深度 diff（透明化变更推理）。中途交互引导。首个被标记"网络安全高能力"的模型。

真正故事：SWE-bench Pro 提升微弱（+0.4%），但 Terminal-Bench（+13.3pp）和 OSWorld（+26.5pp）巨幅跃升——终端调试和真实计算机交互大幅进化。

### Codex 平台

不只是模型——完整代理平台：

```
┌─────────────────────────────────────────────┐
│  CODEX APP（Web + 桌面）                     │
│  指挥中心，并行代理                           │
├─────────────────────────────────────────────┤
│  CODEX CLI（终端）                           │
│  开源 Rust，本地运行                          │
├─────────────────────────────────────────────┤
│  CODEX MODELS（API）                         │
│  codex-1 → GPT-5-Codex → 5.2 → 5.3         │
└─────────────────────────────────────────────┘
```

核心设计：codex-1 = o3 在真实编程任务上通过 RL 微调。沙箱化（无网络）。AGENTS.md 做仓库级配置。内置 git worktree 并行。Automations（定时触发 issue 分类、CI 监控）。

Codex 桌面（2 月 2 日）：macOS 应用，代理独立运行 30 分钟，内置 worktree。Sam Altman："我们有史以来最受欢迎的内部产品。"

---

## 4. Anthropic Claude 深度分析

### Opus 4.6 — 最新（2026 年 2 月 5 日）

| 特性 | Opus 4.5 | Opus 4.6 |
|------|----------|----------|
| 上下文 | 200K | **100 万（beta）** |
| 输出 token | 32K | **128K** |
| Terminal-Bench 2.0 | 59.3% | **65.4%** |
| OSWorld | 66.3% | **72.7%** |
| 长上下文检索 | ~18% | **76%**（4 倍提升） |
| Agent Teams | 无 | **有** |
| 努力程度控制 | 无 | **有 (low/med/high/max)** |

**Agent Teams** — 多个 Claude Code 实例自主协调。主代理生成队友，队友之间直接通信。共享任务列表带依赖追踪。启用：`CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1`。压力测试：16 个代理写了 10 万行 C 编译器，能构建 Linux 6.9。

**发现 500+ 零日漏洞** — 红队在沙箱中给 Opus 4.6 提供 Python、调试器和模糊测试工具。自主发现 500+ 高危未知漏洞（GhostScript 崩溃、OpenSC 缓冲区溢出、CGIF 内存损坏）。

**定价：** $5/$25 每 MTok（与 4.5 相同）。比 Opus 4/4.1 的 $15/$75 降价 67%。

### Sonnet 4.5 — 日常主力

SWE-bench 77.2%（并行计算可达 82%）。最佳速度/质量比。$3/$15 每 MTok。最高 pass@5（55.1%）——唯一能解决其他模型都解不了的实例。

### Haiku 4.5 — 黑马

SWE-bench 73.3%——匹配 Opus 4。$1/$5 每 MTok。比 Sonnet 便宜 5 倍。适合写-测-修循环。

### Claude Code CLI

终端代理，本地执行，直连 API。上线 6 个月内 **10 亿美元年化收入**。2 月 5 日起可在 GitHub Copilot 使用。

核心能力：完整仓库遍历、多文件编辑、MCP 集成（Figma、Jira、GitHub）、会话传送、Chrome 集成、技能热重载、LSP 功能。

开发者转变："我现在更多时间在 reviewer 模式。" "本来 2 年的工作，6 个月高质量完成。"

---

## 5. 代理编码工具对比

### 四大工具

| 特性 | Claude Code | OpenAI Codex | Cursor | GitHub Copilot |
|------|------------|-------------|--------|----------------|
| 界面 | 终端 (CLI) | 云端 + 桌面 (Mac) | VS Code 分支 | IDE 扩展 |
| 多代理 | Agent Teams | 并行 worktree | 单代理 | Agent Mode |
| 执行 | 本地优先 | 云端优先 | 本地 + 云端 | 云端 |
| 自主性 | 高（人在环中） | 非常高（30 分钟） | 中等 | 中等 |
| Token 效率 | 比 Cursor 高 5.5 倍 | 未披露 | 100K-400K/请求 | 未披露 |
| MCP 集成 | 最佳 | 有限 | 模型无关 | GitHub 生态 |
| 模型 | 仅 Claude | 仅 GPT | 多模型 | 多模型 |
| 价格 | $20-200/月 | 有免费层 | $20-200/月 | $10-39/月 |

### 新兴替代

Cline（VS Code，最大灵活性）、Aider（开源终端）、Windsurf（AI IDE）、JetBrains Junie、Gemini CLI、AWS Kiro、Augment（企业）。

---

## 6. 定价全景

### OpenAI API

| 模型 | 输入/MTok | 输出/MTok | 最适场景 |
|------|----------|----------|---------|
| GPT-4.1 nano | $0.10 | $0.40 | 自动补全 |
| GPT-4.1 mini | $0.40 | $1.60 | 日常编程 |
| o4-mini | $1.10 | $4.40 | 竞赛编程 |
| GPT-5 | $1.25 | $10.00 | 通用编程 + 推理 |
| GPT-5.2 | $1.75 | $14.00 | 代理编程 |
| o3（降价后） | $2.00 | $8.00 | 高难推理 |
| o3-pro | $20.00 | $80.00 | 极难问题 |

### Anthropic API

| 模型 | 输入/MTok | 输出/MTok | 最适场景 |
|------|----------|----------|---------|
| Haiku 4.5 | $1 | $5 | 快速修复、迭代 |
| Sonnet 4.5 | $3 | $15 | 日常编程 |
| Opus 4.6 | $5 | $25 | 复杂架构 |
| Opus 4.6 (>200K) | $10 | $37.50 | 大型代码库分析 |

**成本优化：** Prompt 缓存 = 输入省 90%。批量 API = 全部省 50%。两者乘法叠加。

**核心洞察：** 每任务成本 > 每 token 成本。GPT-5 比 o3 少用 22% token。Claude Code 比 Cursor 少用 5.5 倍 token。每 token 最便宜的不一定是每任务最便宜的。

---

## 7. 基准测试深度分析

### SWE-bench Verified 排名（2026 年 2 月）

| 排名 | 模型 | 分数 |
|------|------|------|
| 1 | Claude Opus 4.6 (Thinking) | 80.8% |
| 2 | Claude Opus 4.5 | 80.9%（pass@1: 74.4%） |
| 3 | Gemini 3 Flash | 76.2% |
| 4 | GPT-5.2 | 75.4% |
| 5 | GLM-4.7（开源） | ~74.2% |
| 6 | Qwen3-Coder-Next（开源） | 70.6% |

### Terminal-Bench 2.0（代理 CLI）

| 排名 | 模型 | 分数 |
|------|------|------|
| 1 | GPT-5.3-Codex | **77.3%** |
| 2 | Claude Opus 4.6 | 65.4% |
| 3 | GPT-5.2-Codex | 64.0% |
| 4 | Claude Opus 4.5 | 59.8% |

### OSWorld-Verified（桌面自动化）

| 模型 | 分数 | 人类基线 |
|------|------|---------|
| Claude Opus 4.6 | **72.7%** | ~72% |
| GPT-5.3-Codex | 64.7% | ~72% |

Claude 匹配人类基线。GPT-5.3 比前代跃升 26pp。

### SWE-bench 变体解释

| 变体 | 关注点 | 现实检验 |
|------|--------|---------|
| Verified | 500 人工验证样本 | 黄金标准，最高 ~80% |
| Pro | 企业级，多语言 | 难得多，最高 ~57% |
| Live | 全新题，防污染 | 测试真正泛化 |
| Terminal-Bench 2.0 | 多步终端工作流 | 最接近代理编程实际 |

SWE-bench Pro 分数（23-57%）揭示了与 Verified（~80%）的真实差距。用哪个基准极大影响叙事。

---

## 8. 开发者生态与情绪

### 采用率（2025-2026）

| 指标 | 数值 |
|------|------|
| 定期使用 AI 工具的开发者 | 85% |
| 每周使用 AI 编程助手 | 65% |
| AI 用户中使用 ChatGPT | 82% |
| Copilot 使用率 | 68% |
| Gemini 使用率 | 47% |
| Claude 使用率 | 41% |
| 使用多个工具 | 26%+ |
| Codex 用户（2025 年 12 月起） | 100 万+ |

### METR 重磅研究

随机对照试验——16 名资深开发者，246 个任务：
- AI 工具让开发者在熟悉代码库上**慢了 19%**
- 开发者自估 AI 节省 20-24%——**与现实完全相反**
- 越熟悉代码库的开发者被拖慢越多
- 背景：2025 年初工具（Cursor Pro、Claude 3.5/3.7 Sonnet）。新模型可能不同。

**悖论：** AI 在你不了解代码库时最有价值——而这恰恰是你最想用它的时候。

### 社区工作流共识

没有单一最优工具。多工具策略主导：
- **Cursor** — 内联编辑、快速迭代（基线）
- **Claude Code** — 复杂推理、调试、架构（"最强编码大脑"）
- **Copilot** — 快速补全、低摩擦
- **Codex** — 并行委托、多代理（"最佳 PR 审查器"）
- **Cline** — 追求最大控制的高级用户

### 编码-写作权衡（Opus 4.6）

用户报告编码提升但写作下降。共识：编码用 Opus 4.6，散文用 Opus 4.5。Anthropic 重度优化推理基准——效果可见。

---

## 9. 非显而易见的洞察

### 1. 基准碎片化是策略
两家报告不同 SWE-bench 变体。聪明的消费者看 Terminal-Bench 2.0 和 OSWorld——两家都在这些上报告。

### 2. 开源快速追赶
Qwen3-Coder-Next（80B 中 3B 活跃）SWE-bench 70.6%。前沿闭源与最佳开源差距从 30+ 分缩小到不到 10 分。

### 3. 多代理是新前沿
单代理编码在商品化。差异化转向编排：Claude Agent Teams（群体自组织）vs Codex 桌面（并行 worktree、30 分钟自主）。

### 4. Token 效率是隐藏战场
Claude Code 比 Cursor 少用 5.5 倍 token。GPT-5 比 o3 少用 22%。规模化时这比每 token 定价更重要。

### 5. 企业谨慎
90% 企业团队使用 AI，但 AI PR 审查等待时间增加 4.6 倍。积极情绪从 70% 降到 60%。66% 开发者引用"几乎正确但不完全正确"问题。

---

## 10. 可偷用的模式

| 模式 | 描述 |
|------|------|
| **模型路由** | Haiku/nano 迭代，Sonnet/GPT-5 日常，Opus/Codex 架构 |
| **AGENTS.md / CLAUDE.md** | 仓库级代理配置——最高影响力的质量提升 |
| **每代理一个 worktree** | 每个代理独立 git worktree，无冲突并行 |
| **多工具工作流** | Cursor（内联）+ Claude Code（复杂）+ Copilot（补全）+ Codex（并行） |
| **按任务跟踪成本** | 追踪每完成任务的总 token，而非每 token 成本 |
| **沙箱执行** | 代理无网络，预装依赖，限制供应链风险 |
| **上下文压缩** | 长代理会话的摘要检查点 |
| **Prompt 缓存** | 输入省 90%，与批量 API 乘法叠加 |
| **Terminal-Bench 评估** | 比 SWE-bench Verified 更真实的代理评估 |
| **Codex 做 PR 审查** | 即使 Claude 为主的团队也应用 Codex PR 审查 |
| **版本特定选择** | Opus 4.6 编码，Opus 4.5 写作/文档 |
| **努力程度控制** | `/effort low` 简单补全，`/effort max` 难题 |

---

## 最终判决：谁赢了？

**没有单一赢家。**

| 类别 | 领先者 | 差距 |
|------|--------|------|
| SWE-bench Verified | Claude Opus 4.6 | 比 GPT-5.2 领先约 5pp |
| Terminal-Bench 2.0 | GPT-5.3-Codex | 比 Claude 领先约 12pp |
| OSWorld（桌面） | Claude Opus 4.6 | 比 GPT-5.3 领先约 8pp |
| 推理（GPQA、MMLU） | Claude Opus 4.6 | 约 2-4pp |
| 速度 | GPT-5.3-Codex | 推理快 25% |
| 上下文窗口 | Claude Opus 4.6 | 100 万 vs 40 万 |
| 多代理编排 | 平手 | 不同方法 |
| IDE 集成 | Cursor | 使用两家模型 |
| 企业采用 | GitHub Copilot | 市场主导 |
| 开源替代 | Qwen3-Coder-Next | 70.6%，成本零头 |
| 开发者心智（编码） | Claude | "最强编码大脑" |
| 开发者心智（速度） | Codex | "快速迭代" |
| Token 效率 | Claude Code | Cursor 的 5.5 倍 |
